---
layout: single
date: 2021-04-15
title: "한국어 임베딩 BERT"
description: "BERT description"
comments: true
published: true
typora-root-url: ../
categories: "NLP"
toc: true
toc_sticky: true 
---

# BERT 매커니즘

## 1.1 다양한 임베딩 방식들



### 1.1.1 Word2vec 

* 자연어(특히, 단어)의 의미를 벡터 공간에 임베딩.

* 한 단어의 주변 단어들을 통해 해당 단어의 의미를 파악.

* 장점: 
  *  단어간의 유사도 측정하기 용이
  * 단어간 관계 파악에 용이
  * 벡터 연산을 통한 추론 가능(e.g. 한국 - 서울 +도쿄  = ?) 

* 단점:
  * 단어의 subword information무시(이를 보완한 것이 n-gram이용한 fasttext)
  * OOV(out of vocabulary)에서 적용 불가능



### 1.1.2 FASTTEXT

한국어는 다양한 용언 형태를 가진다.(e.g. 모르네, 모르데, 모르지, 모르구나, ***)

Word2Vec의 경우 이러한 다양한 용언 형태를 모두 각각 다른 어휘로 인식한다는 한계를 가짐.

---

<용언이 변화함에도 그 의미는 비슷하다는 것을 어떻게 알 수 있을까?>

* Fasttext는 단어를 n-gram으로 나누어 이를 모두 학습시킴으로서 subtext에 대한 인식 가능성을 높임(e.g. assumption을 학습시킨다고 할 경우 'as', 'ss','su', ***, 'assumption'으로 n-gram vectors로 쪼개어 임베딩. 결과적으로 모델은 assump이라는 어휘와 assumption이 별개의 말이 아니라는 것을 인식할 수 있음)
* Fasttext 장점:
  * 오탈자, OOV, 등장 횟수가 적은 학습 단어에 대해서도 학습 효과가 좋다

* Word2Vec나 FastText의 단점:
  * 동형어, 다의어 등에 대해서는 embedding 성능이 좋지 못함.
  * 주변 ''단어''를 통해 학습이 이루어지기에 '문맥'을 고려할 수 없음.

### 1.1.3 기계 독해와 BERT

Bert로 인해 기계 독해 분야가 빠르게 성장하기 시작.

기계 독해란? 한 마디로 Semantic Analysis. 의미를 분석하는 것. 기계가 글을 읽어 사람이 사전에 질문들에 대한 답을 미리 입력해 주지 않아도 기계가 알아서 질문과 글을 이해하고 질문에 적절한 답을 제공해 줄 수 있음.

의미 해석은 결국 분류의 문제이다. ex) 최초의 컴퓨터가 뭐야? <- 질문: 98%, 요구: 0.5%, 거절: 0.025%, 승낙: 0.025%



## 1.2 워드 임베딩의 활용

* 토픽 키워드 추출할 때 활용
* 다른 자연어처리 모델의 입력으로 사용(e.g. 학습 데이터의 양이 매우 적은 감성분석을 사용할 경우, 학습 데이터만으로는 특성 추출 불가)